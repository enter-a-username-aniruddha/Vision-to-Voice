{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjVQUcN9G9sr",
    "outputId": "f6071924-ce9d-475f-ef47-65d458596fd7"
   },
   "outputs": [],
   "source": [
    "# prompt: mount drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2PT9ixQHd3z",
    "outputId": "10dce2ff-7518-4c62-d797-4e58e17664dd"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/Gohan_Dataset/RT3_Dataset/train.zip -d /content/RT3_Dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuUnTTyUHd6E",
    "outputId": "8cc7bfcd-7cc8-4d61-f3d2-0825d5dfa906"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/Gohan_Dataset/RT3_Dataset/valid.zip -d /content/RT3_Dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I36aeOsfHd8T",
    "outputId": "709f4c5c-0a5e-40ec-a448-b7b92cc1779c"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ME7UiLAOHd_2",
    "outputId": "9fe2e25e-347e-4a67-8eb7-e25239ed4266"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eACJxI_sH3U0",
    "outputId": "7904a93c-288e-4e8e-89fc-d2e59c0f29b1"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train(data='/content/drive/MyDrive/Gohan_Dataset/RT3_Dataset/data.yaml' , epochs=4, batch=24, imgsz=640, name=\"yolov8_custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFikVjjHalLm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "7Vx_QP6palOb",
    "outputId": "f5088ffa-db3a-4cc4-9e7c-709165eab0ee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkqAOaPA_yhQ"
   },
   "source": [
    "# Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d8QALl0alQ-",
    "outputId": "18915a5e-7ddd-443d-f74c-ec9713dd7738"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "\n",
    "!pip install gTTS\n",
    "\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "id": "6pnlEyi1CoYc",
    "outputId": "9ec27427-cc8b-4111-d92a-5e082ab19649"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow  # Use cv2_imshow in Colab\n",
    "\n",
    "# Load the custom-trained YOLO model\n",
    "model = YOLO('/content/drive/MyDrive/yolov8_custom/weights/best.pt')  # Update to your model path\n",
    "\n",
    "def detect_objects(image_path, confidence=0.5):\n",
    "    \"\"\"Detect objects in the image and return results.\"\"\"\n",
    "    try:\n",
    "        results = model.predict(source=image_path, save=True, conf=confidence)\n",
    "        if not results:\n",
    "            print(\"No objects detected.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during object detection: {e}\")\n",
    "        return None\n",
    "\n",
    "def count_objects(results):\n",
    "    \"\"\"Count detected objects by class.\"\"\"\n",
    "    detected_classes = results[0].names  # Get class names\n",
    "    counts = defaultdict(int)  # Dictionary to hold object counts\n",
    "\n",
    "    # Count each detected object\n",
    "    for box in results[0].boxes:\n",
    "        cls = int(box.cls)\n",
    "        class_name = detected_classes[cls]\n",
    "        counts[class_name] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def generate_count_feedback(counts):\n",
    "    \"\"\"Generate text summary of object counts and convert it to speech.\"\"\"\n",
    "    # Generate the feedback text in the format: 'Detected objects are 3 person, 2 cars, 1 bike'\n",
    "    feedback_texts = [f\"{count} {name}\" for name, count in counts.items()]\n",
    "    combined_feedback = \"Detected objects are \" + \", \".join(feedback_texts) + \".\"\n",
    "\n",
    "    # Convert to audio using gTTS\n",
    "    tts = gTTS(text=combined_feedback, lang='en', slow=False)\n",
    "    tts.save(\"count_feedback.wav\")\n",
    "    return Audio(\"count_feedback.wav\", autoplay=True)\n",
    "\n",
    "# Example usage\n",
    "image_path = '/content/gettyimages-517928072-1024x1024.jpg'  # Replace with your image path\n",
    "results = detect_objects(image_path)\n",
    "\n",
    "if results:\n",
    "    # Show the image with bounding boxes\n",
    "    cv2_imshow(results[0].plot())  # This will display the image with bounding boxes\n",
    "\n",
    "    # Count detected objects and generate feedback\n",
    "    counts = count_objects(results)\n",
    "    audio_output = generate_count_feedback(counts)\n",
    "    display(audio_output)\n",
    "else:\n",
    "    print(\"No results to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "OnStHhCoalT2",
    "outputId": "becb66bc-6c31-45ae-c476-9f3d7a26b625"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your custom-trained YOLO model\n",
    "model = YOLO('/content/drive/MyDrive/yolov8_custom/weights/best.pt')  # Change this to 'last.pt' if needed\n",
    "\n",
    "# Predict objects in an image\n",
    "results = model.predict(source='/content/depositphotos_356489422-stock-photo-new-delhi-india-nobember-2019.jpg', save=True, conf=0.30)  # Adjust confidence threshold as needed\n",
    "\n",
    "# Display results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "id": "UJAXqwaSbUfg",
    "outputId": "b43a8b1b-f407-4e66-cd34-5baf4ba1331d"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display\n",
    "import cv2\n",
    "\n",
    "# Load your custom-trained YOLO model\n",
    "model = YOLO('/content/drive/MyDrive/yolov8_custom/weights/best.pt')  # Adjust path to your model\n",
    "\n",
    "# Define object size estimates and class category mapping\n",
    "object_size_estimates = {\n",
    "    'person': 1.7, 'animal': 1.0, 'small_object': 0.2, 'medium_object': 0.5,\n",
    "    'large_object': 1.5, 'vehicle': 1.5, 'large_vehicle': 3.0\n",
    "}\n",
    "\n",
    "class_category_map = {\n",
    "    'person': 'person', 'dog': 'animal', 'cat': 'animal', 'elephant': 'animal', 'giraffe': 'animal',\n",
    "    'bottle': 'small_object', 'cup': 'small_object', 'remote': 'small_object', 'cell phone': 'small_object',\n",
    "    'laptop': 'medium_object', 'keyboard': 'medium_object', 'book': 'small_object', 'pottedplant': 'medium_object',\n",
    "    'bicycle': 'vehicle', 'motorbike': 'vehicle', 'car': 'vehicle', 'bus': 'large_vehicle', 'train': 'large_vehicle',\n",
    "    'truck': 'large_vehicle', 'aeroplane': 'large_vehicle', 'boat': 'large_vehicle'\n",
    "}\n",
    "\n",
    "def calculate_distance(real_size, focal_length, bbox_dimension):\n",
    "    return (real_size * focal_length) / bbox_dimension\n",
    "\n",
    "# Set camera focal length\n",
    "focal_length = 500  # Approximate focal length in pixels\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    try:\n",
    "        # Perform object detection\n",
    "        results = model.predict(source=image_path, save=True, conf=0.5)  # Adjust confidence threshold as needed\n",
    "\n",
    "        # Check if results are obtained\n",
    "        if results:\n",
    "            # Display the image with bounding boxes\n",
    "            results[0].show()  # Shows the first result with bounding boxes\n",
    "        else:\n",
    "            print(\"No objects detected.\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during object detection: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_distances(results):\n",
    "    detected_classes = results[0].names  # Class names from YOLO results\n",
    "    detected_distances = []\n",
    "\n",
    "    # Iterate over detected objects and calculate distances\n",
    "    for box in results[0].boxes:\n",
    "        cls = int(box.cls)  # Class index\n",
    "        class_name = detected_classes[cls]  # Get the name of the detected class\n",
    "\n",
    "        # Get category and size estimate for the object based on the class\n",
    "        category = class_category_map.get(class_name, 'medium_object')\n",
    "        real_size = object_size_estimates[category]\n",
    "\n",
    "        # Get bounding box coordinates in xyxy format\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "\n",
    "        # Calculate bbox height and width in pixels\n",
    "        bbox_height = (y2 - y1).item()\n",
    "        bbox_width = (x2 - x1).item()\n",
    "\n",
    "        # Determine dimension to use for distance calculation based on object category\n",
    "        if category in ['person', 'animal']:  # For tall objects, use height\n",
    "            distance = calculate_distance(real_size, focal_length, bbox_height)\n",
    "        else:  # For wide objects (vehicles, etc.), use width\n",
    "            distance = calculate_distance(real_size, focal_length, bbox_width)\n",
    "\n",
    "        # Store detected object information with calculated distance and position\n",
    "        detected_distances.append((class_name, distance, (x1, y1, x2, y2)))\n",
    "\n",
    "    # Sort detected objects by distance\n",
    "    detected_distances.sort(key=lambda x: x[1])  # Sort by distance in ascending order\n",
    "\n",
    "    # Extract nearest, medium, and farthest objects\n",
    "    nearest_object = detected_distances[0] if detected_distances else None\n",
    "    farthest_object = detected_distances[-1] if detected_distances else None\n",
    "    medium_index = len(detected_distances) // 2\n",
    "    medium_object = detected_distances[medium_index] if detected_distances else None\n",
    "\n",
    "    return nearest_object, medium_object, farthest_object, detected_distances\n",
    "\n",
    "def generate_audio_feedback(nearest_object, medium_object, farthest_object):\n",
    "    feedback_texts = []\n",
    "\n",
    "    def get_direction(x_center):\n",
    "        \"\"\"Determine direction based on x-center relative to image center (320 for 640px width).\"\"\"\n",
    "        if x_center < 320:\n",
    "            return \"left\"\n",
    "        elif x_center > 320:\n",
    "            return \"right\"\n",
    "        return \"center\"\n",
    "\n",
    "    # Get direction and feedback for the nearest object\n",
    "    if nearest_object:\n",
    "        feedback_texts.append(f\"The nearest object is a {nearest_object[0]} at {nearest_object[1]:.2f} meters.\")\n",
    "\n",
    "        # Determine direction to nearest object based on bounding box coordinates\n",
    "        x1, y1, x2, y2 = nearest_object[2]\n",
    "        nearest_direction = get_direction((x1 + x2) / 2)\n",
    "        feedback_texts.append(f\"It is to your {nearest_direction}.\")\n",
    "    else:\n",
    "        feedback_texts.append(\"No nearest object detected.\")\n",
    "\n",
    "    # Get direction and feedback for the farthest object\n",
    "    if farthest_object:\n",
    "        feedback_texts.append(f\"The farthest object is a {farthest_object[0]} at {farthest_object[1]:.2f} meters.\")\n",
    "\n",
    "        # Determine direction to farthest object based on bounding box coordinates\n",
    "        x1, y1, x2, y2 = farthest_object[2]\n",
    "        farthest_direction = get_direction((x1 + x2) / 2)\n",
    "        feedback_texts.append(f\"It is to your {farthest_direction}.\")\n",
    "    else:\n",
    "        feedback_texts.append(\"No farthest object detected.\")\n",
    "\n",
    "    # Safety direction suggestion\n",
    "    if nearest_object and nearest_direction == \"left\":\n",
    "        feedback_texts.append(\"It is safer to move to the right.\")\n",
    "    elif nearest_object and nearest_direction == \"right\":\n",
    "        feedback_texts.append(\"It is safer to move to the left.\")\n",
    "    else:\n",
    "        feedback_texts.append(\"You are facing the object, proceed with caution.\")\n",
    "\n",
    "    # Add medium object feedback\n",
    "    if medium_object:\n",
    "        feedback_texts.append(f\"The middle object is a {medium_object[0]} at {medium_object[1]:.2f} meters. Be aware of that.\")\n",
    "    else:\n",
    "        feedback_texts.append(\"No middle object detected.\")\n",
    "\n",
    "    # Combine the feedback texts\n",
    "    combined_feedback = \" \".join(feedback_texts)\n",
    "\n",
    "    # Convert text to speech using gTTS\n",
    "    gtts_object = gTTS(text=combined_feedback, lang='en', slow=False)\n",
    "    gtts_object.save(\"detection_feedback.wav\")\n",
    "\n",
    "    # Play the audio\n",
    "    return Audio(\"detection_feedback.wav\", autoplay=True)\n",
    "\n",
    "# Example usage\n",
    "image_path = '/content/-1x-1.webp'  # Replace with your test image path\n",
    "results = detect_objects(image_path)  # Detect objects and show image\n",
    "\n",
    "if results:  # Ensure results are obtained before processing distances\n",
    "    nearest_object, medium_object, farthest_object, detected_distances = process_distances(results)  # Process distances\n",
    "    audio_output = generate_audio_feedback(nearest_object, medium_object, farthest_object)  # Generate audio feedback\n",
    "\n",
    "    # Display the audio output\n",
    "    display(audio_output)\n",
    "else:\n",
    "    print(\"No results to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ_zW20v8iQS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vxa2RJVK95rs",
    "outputId": "13ecedb1-f6d1-4234-cba4-ba7b68895ed0"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow  # Use cv2_imshow for Colab compatibility\n",
    "\n",
    "# Load your custom-trained YOLO model\n",
    "model = YOLO('/content/drive/MyDrive/yolov8_custom/weights/best.pt')  # Adjust path to your model\n",
    "\n",
    "# Define object size estimates and class category mapping\n",
    "object_size_estimates = {\n",
    "    'person': 1.7, 'animal': 1.0, 'small_object': 0.2, 'medium_object': 0.5,\n",
    "    'large_object': 1.5, 'vehicle': 1.5, 'large_vehicle': 3.0\n",
    "}\n",
    "\n",
    "class_category_map = {\n",
    "    'person': 'person', 'dog': 'animal', 'cat': 'animal', 'elephant': 'animal', 'giraffe': 'animal',\n",
    "    'bottle': 'small_object', 'cup': 'small_object', 'remote': 'small_object', 'cell phone': 'small_object',\n",
    "    'laptop': 'medium_object', 'keyboard': 'medium_object', 'book': 'small_object', 'pottedplant': 'medium_object',\n",
    "    'bicycle': 'vehicle', 'motorbike': 'vehicle', 'car': 'vehicle', 'bus': 'large_vehicle', 'train': 'large_vehicle',\n",
    "    'truck': 'large_vehicle', 'aeroplane': 'large_vehicle', 'boat': 'large_vehicle'\n",
    "}\n",
    "\n",
    "def calculate_distance(real_size, focal_length, bbox_dimension):\n",
    "    return (real_size * focal_length) / bbox_dimension\n",
    "\n",
    "# Set camera focal length\n",
    "focal_length = 500  # Approximate focal length in pixels\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Process each frame to detect objects and annotate them with bounding boxes and distances.\"\"\"\n",
    "    results = model.predict(source=frame, conf=0.5)\n",
    "    detected_classes = results[0].names\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        cls = int(box.cls)\n",
    "        class_name = detected_classes[cls]\n",
    "\n",
    "        # Get size estimate for the object class\n",
    "        category = class_category_map.get(class_name, 'medium_object')\n",
    "        real_size = object_size_estimates[category]\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        bbox_height = (y2 - y1).item()\n",
    "        bbox_width = (x2 - x1).item()\n",
    "\n",
    "        # Calculate distance based on object dimensions\n",
    "        if category in ['person', 'animal']:\n",
    "            distance = calculate_distance(real_size, focal_length, bbox_height)\n",
    "        else:\n",
    "            distance = calculate_distance(real_size, focal_length, bbox_width)\n",
    "\n",
    "        # Annotate frame with bounding box and distance label\n",
    "        label = f\"{class_name} {distance:.2f}m\"\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"Process the input video, annotate frames, and save the output.\"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define video codec and writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of video or failed to grab frame.\")\n",
    "                break\n",
    "\n",
    "            # Process the frame and annotate it\n",
    "            annotated_frame = process_frame(frame)\n",
    "\n",
    "            # Write the processed frame to the output video\n",
    "            out.write(annotated_frame)\n",
    "\n",
    "            # Optional: Display the frame in Colab for real-time preview\n",
    "            cv2_imshow(annotated_frame)\n",
    "\n",
    "            # To exit loop by pressing 'q' in a non-Colab environment, uncomment below\n",
    "            # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #     break\n",
    "\n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Path to input and output video files\n",
    "input_video_path = '/content/WhatsApp Video 2024-11-09 at 10.59.24 PM.mp4'  # Path to your input video\n",
    "output_video_path = '/content/output_video_with_bboxes.mp4'  # Path where output video will be saved\n",
    "\n",
    "# Process and save the video\n",
    "process_video(input_video_path, output_video_path)\n",
    "print(\"Processing complete. Output video saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMBz0JqV95vE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEFote6AxwNZ"
   },
   "source": [
    "# RUN THIS FOLLOWING CODE ON JUPYTER NOTEBOOK AS COLAB DOESN'T SUPPORTS HARDWARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t78TCnIrVU25"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the specified path as the current working directory\n",
    "os.chdir(\"C:\\\\Users\\\\Aniruddha\\\\Desktop\\\\Vision_To_Voice\\\\YOLOv8nano_TFLite_model\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSYcwIJgVU5x"
   },
   "outputs": [],
   "source": [
    "pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNmSBu0pVU82"
   },
   "outputs": [],
   "source": [
    "!pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QURBPWYbVVAO"
   },
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load your custom-trained YOLO model\n",
    "model = YOLO(r\"C:\\Users\\Aniruddha\\Desktop\\Vision_To_Voice\\YOLOv8nano_TFLite_model\\runs\\detect\\yolov8_custom\\weights\\best.pt\")\n",
    "\n",
    "# Define object size estimates and class category mapping\n",
    "object_size_estimates = {\n",
    "    'person': 1.7, 'animal': 1.0, 'small_object': 0.2, 'medium_object': 0.5,\n",
    "    'large_object': 1.5, 'vehicle': 1.5, 'large_vehicle': 3.0\n",
    "}\n",
    "\n",
    "class_category_map = {\n",
    "    'person': 'person', 'dog': 'animal', 'cat': 'animal', 'elephant': 'animal', 'giraffe': 'animal',\n",
    "    'bottle': 'small_object', 'cup': 'small_object', 'remote': 'small_object', 'cell phone': 'small_object',\n",
    "    'laptop': 'medium_object', 'keyboard': 'medium_object', 'book': 'small_object', 'pottedplant': 'medium_object',\n",
    "    'bicycle': 'vehicle', 'motorbike': 'vehicle', 'car': 'vehicle', 'bus': 'large_vehicle', 'train': 'large_vehicle',\n",
    "    'truck': 'large_vehicle', 'aeroplane': 'large_vehicle', 'boat': 'large_vehicle'\n",
    "}\n",
    "\n",
    "def calculate_distance(real_size, focal_length, bbox_dimension):\n",
    "    return (real_size * focal_length) / bbox_dimension\n",
    "\n",
    "# Set camera focal length\n",
    "focal_length = 500  # Approximate focal length in pixels\n",
    "\n",
    "def process_frame(frame, announced_objects):\n",
    "    \"\"\"Detect and annotate objects in a single frame with bounding boxes and distance labels.\"\"\"\n",
    "    results = model.predict(source=frame, conf=0.5)\n",
    "    detected_classes = results[0].names\n",
    "    current_distances = {}\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        cls = int(box.cls)\n",
    "        class_name = detected_classes[cls]\n",
    "\n",
    "        # Get size estimate for the object class\n",
    "        category = class_category_map.get(class_name, 'medium_object')\n",
    "        real_size = object_size_estimates[category]\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        bbox_height = (y2 - y1).item()\n",
    "        bbox_width = (x2 - x1).item()\n",
    "\n",
    "        # Calculate distance based on object dimensions\n",
    "        if category in ['person', 'animal']:\n",
    "            distance = calculate_distance(real_size, focal_length, bbox_height)\n",
    "        else:\n",
    "            distance = calculate_distance(real_size, focal_length, bbox_width)\n",
    "\n",
    "        # Only consider the closest instance for each object type\n",
    "        if class_name not in current_distances or distance < current_distances[class_name]:\n",
    "            current_distances[class_name] = distance\n",
    "\n",
    "        # Annotate frame with bounding box and distance label\n",
    "        label = f\"{class_name} {distance:.2f}m\"\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Update announcements based on the closest distance per object type\n",
    "    for class_name, distance in current_distances.items():\n",
    "        if distance < 5 and (class_name not in announced_objects or distance < announced_objects[class_name]):\n",
    "            # Update or announce only if a closer distance is detected\n",
    "            direction = \"left\" if (x1 + x2) / 2 < frame.shape[1] / 2 else \"right\"\n",
    "            audio_text = f\"{class_name} detected at {distance:.1f} meters. Move {direction}.\"\n",
    "            announce_object(audio_text)\n",
    "            announced_objects[class_name] = distance  # Update with the new closest distance\n",
    "\n",
    "    return frame, announced_objects\n",
    "\n",
    "def announce_object(text):\n",
    "    \"\"\"Convert text to speech using gTTS and play it.\"\"\"\n",
    "    tts = gTTS(text=text, lang='en', slow=False)\n",
    "    tts.save(\"detection_feedback.wav\")\n",
    "    display(Audio(\"detection_feedback.wav\", autoplay=True))\n",
    "\n",
    "def live_object_detection():\n",
    "    \"\"\"Perform real-time object detection using webcam.\"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # 0 for the default camera; use 1 or other indices for external webcams\n",
    "    announced_objects = {}  # Dictionary to track announced objects\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame.\")\n",
    "                break\n",
    "\n",
    "            # Process the frame for object detection and distance annotation\n",
    "            annotated_frame, announced_objects = process_frame(frame, announced_objects)\n",
    "\n",
    "            # Display the frame with annotations\n",
    "            cv2.imshow(\"Live Object Detection\", annotated_frame)\n",
    "\n",
    "            # Press 'q' to quit the live stream\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
